{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Person of Interest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed the following outliers:\n",
      "TOTAL\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "\n",
      "The total number of people in the dataset after removing outliers: 144\n",
      "Total Persons of Interest: 18\n",
      "Total who are not Persons of Interest: 126\n",
      "\n",
      "TOTAL NAN VALUES BY LABEL:\n",
      "\n",
      "poi : 0\n",
      "total_stock_value : 19\n",
      "total_payments : 21\n",
      "email_address : 33\n",
      "restricted_stock : 35\n",
      "exercised_stock_options : 43\n",
      "salary : 50\n",
      "expenses : 50\n",
      "other : 53\n",
      "to_messages : 58\n",
      "from_poi_to_this_person : 58\n",
      "from_messages : 58\n",
      "from_this_person_to_poi : 58\n",
      "shared_receipt_with_poi : 58\n",
      "bonus : 63\n",
      "long_term_incentive : 79\n",
      "deferred_income : 96\n",
      "deferral_payments : 106\n",
      "restricted_stock_deferred : 127\n",
      "director_fees : 128\n",
      "loan_advances : 141\n",
      "\n",
      "Dataset after cleaning data and adding new features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 144 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 25 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   salary                       144 non-null    float64\n",
      " 1   to_messages                  144 non-null    float64\n",
      " 2   deferral_payments            144 non-null    float64\n",
      " 3   total_payments               144 non-null    float64\n",
      " 4   loan_advances                144 non-null    float64\n",
      " 5   bonus                        144 non-null    float64\n",
      " 6   email_address                111 non-null    object \n",
      " 7   restricted_stock_deferred    144 non-null    float64\n",
      " 8   deferred_income              144 non-null    float64\n",
      " 9   total_stock_value            144 non-null    float64\n",
      " 10  expenses                     144 non-null    float64\n",
      " 11  from_poi_to_this_person      144 non-null    float64\n",
      " 12  exercised_stock_options      144 non-null    float64\n",
      " 13  from_messages                144 non-null    float64\n",
      " 14  other                        144 non-null    float64\n",
      " 15  from_this_person_to_poi      144 non-null    float64\n",
      " 16  poi                          144 non-null    int32  \n",
      " 17  long_term_incentive          144 non-null    float64\n",
      " 18  shared_receipt_with_poi      144 non-null    float64\n",
      " 19  restricted_stock             144 non-null    float64\n",
      " 20  director_fees                144 non-null    float64\n",
      " 21  total_restricted_stock_diff  144 non-null    float64\n",
      " 22  to_poi_ratio                 144 non-null    float64\n",
      " 23  from_poi_ratio               144 non-null    float64\n",
      " 24  payments_to_stock_ratio      144 non-null    float64\n",
      "dtypes: float64(23), int32(1), object(1)\n",
      "memory usage: 28.7+ KB\n",
      "\n",
      "Sample of data after data cleaning, and prior to final feature selection and standardization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_restricted_stock_diff</th>\n",
       "      <th>to_poi_ratio</th>\n",
       "      <th>from_poi_ratio</th>\n",
       "      <th>payments_to_stock_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>METTS MARK</th>\n",
       "      <td>0</td>\n",
       "      <td>365788.0</td>\n",
       "      <td>807.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1061827.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>585062.0</td>\n",
       "      <td>94299.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>585062.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>1.814897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>0</td>\n",
       "      <td>267102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELLIOTT STEVEN</th>\n",
       "      <td>0</td>\n",
       "      <td>170941.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400729.0</td>\n",
       "      <td>6678735.0</td>\n",
       "      <td>78552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4890344.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12961.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1788391.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4890344.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CORDES WILLIAM R</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1038185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>651850.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>386335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>651850.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANNON KEVIN P</th>\n",
       "      <td>1</td>\n",
       "      <td>243293.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3117011.0</td>\n",
       "      <td>6391065.0</td>\n",
       "      <td>34039.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5538001.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11350.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1617011.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>853064.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5538001.0</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  poi    salary  to_messages  deferral_payments  \\\n",
       "name                                                              \n",
       "METTS MARK          0  365788.0        807.0                0.0   \n",
       "BAXTER JOHN C       0  267102.0          0.0          1295738.0   \n",
       "ELLIOTT STEVEN      0  170941.0          0.0                0.0   \n",
       "CORDES WILLIAM R    0       0.0        764.0                0.0   \n",
       "HANNON KEVIN P      1  243293.0       1045.0                0.0   \n",
       "\n",
       "                  total_payments  loan_advances      bonus  \\\n",
       "name                                                         \n",
       "METTS MARK             1061827.0            0.0   600000.0   \n",
       "BAXTER JOHN C          5634343.0            0.0  1200000.0   \n",
       "ELLIOTT STEVEN          211725.0            0.0   350000.0   \n",
       "CORDES WILLIAM R             0.0            0.0        0.0   \n",
       "HANNON KEVIN P          288682.0            0.0  1500000.0   \n",
       "\n",
       "                  restricted_stock_deferred  deferred_income  \\\n",
       "name                                                           \n",
       "METTS MARK                              0.0              0.0   \n",
       "BAXTER JOHN C                           0.0       -1386055.0   \n",
       "ELLIOTT STEVEN                          0.0        -400729.0   \n",
       "CORDES WILLIAM R                        0.0              0.0   \n",
       "HANNON KEVIN P                          0.0       -3117011.0   \n",
       "\n",
       "                  total_stock_value  expenses  from_poi_to_this_person  \\\n",
       "name                                                                     \n",
       "METTS MARK                 585062.0   94299.0                     38.0   \n",
       "BAXTER JOHN C            10623258.0   11200.0                      0.0   \n",
       "ELLIOTT STEVEN            6678735.0   78552.0                      0.0   \n",
       "CORDES WILLIAM R          1038185.0       0.0                     10.0   \n",
       "HANNON KEVIN P            6391065.0   34039.0                     32.0   \n",
       "\n",
       "                  exercised_stock_options  from_messages      other  \\\n",
       "name                                                                  \n",
       "METTS MARK                            0.0           29.0     1740.0   \n",
       "BAXTER JOHN C                   6680544.0            0.0  2660303.0   \n",
       "ELLIOTT STEVEN                  4890344.0            0.0    12961.0   \n",
       "CORDES WILLIAM R                 651850.0           12.0        0.0   \n",
       "HANNON KEVIN P                  5538001.0           32.0    11350.0   \n",
       "\n",
       "                  from_this_person_to_poi  long_term_incentive  \\\n",
       "name                                                             \n",
       "METTS MARK                            1.0                  0.0   \n",
       "BAXTER JOHN C                         0.0            1586055.0   \n",
       "ELLIOTT STEVEN                        0.0                  0.0   \n",
       "CORDES WILLIAM R                      0.0                  0.0   \n",
       "HANNON KEVIN P                       21.0            1617011.0   \n",
       "\n",
       "                  shared_receipt_with_poi  restricted_stock  director_fees  \\\n",
       "name                                                                         \n",
       "METTS MARK                          702.0          585062.0            0.0   \n",
       "BAXTER JOHN C                         0.0         3942714.0            0.0   \n",
       "ELLIOTT STEVEN                        0.0         1788391.0            0.0   \n",
       "CORDES WILLIAM R                     58.0          386335.0            0.0   \n",
       "HANNON KEVIN P                     1035.0          853064.0            0.0   \n",
       "\n",
       "                  total_restricted_stock_diff  to_poi_ratio  from_poi_ratio  \\\n",
       "name                                                                          \n",
       "METTS MARK                                0.0      0.001239        1.310345   \n",
       "BAXTER JOHN C                       6680544.0      0.000000        0.000000   \n",
       "ELLIOTT STEVEN                      4890344.0      0.000000        0.000000   \n",
       "CORDES WILLIAM R                     651850.0      0.000000        0.833333   \n",
       "HANNON KEVIN P                      5538001.0      0.020096        1.000000   \n",
       "\n",
       "                  payments_to_stock_ratio  \n",
       "name                                       \n",
       "METTS MARK                       1.814897  \n",
       "BAXTER JOHN C                    0.530378  \n",
       "ELLIOTT STEVEN                   0.031701  \n",
       "CORDES WILLIAM R                 0.000000  \n",
       "HANNON KEVIN P                   0.045170  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and clean the data, add new features, split into testing/training subsets\n",
    "\n",
    "# Import libraries\n",
    "import pickle\n",
    "from tester import dump_classifier_and_data\n",
    "from dos_to_unix import d2ux\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, make_scorer, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "# Remove obvious outliers (These were discovered by reviewing enron61702insiderpay.pdf)\n",
    "def remove_outliers(data):\n",
    "    data.pop('TOTAL', 0 ) # Remove TOTAL \n",
    "    data.pop('THE TRAVEL AGENCY IN THE PARK', 0) # Remove THE TRAVEL AGENCY IN THE PARK due to lack of relevance\n",
    "    print('Removed the following outliers:')\n",
    "    print('TOTAL')\n",
    "    print('THE TRAVEL AGENCY IN THE PARK')\n",
    "    \n",
    "# Read data to dataframe\n",
    "def to_dataframe(data):\n",
    "    \"\"\"\n",
    "    Create pandas dataframe from dictionary of enron data\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(data).T\n",
    "    dataframe.index = dataframe.index.rename('name')\n",
    "    return dataframe\n",
    "\n",
    "def new_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Create the following new columns:\n",
    "    \n",
    "    total_restricted_stock_diff: total stock value - restricted stock\n",
    "    to_poi_ratio - ratio of emails to poi\n",
    "    from_poi_ratio - ratio of emails from poi\n",
    "    payments_to_stock_ratio - ratio of total payments to total stock value\n",
    "    \"\"\"\n",
    "    dataframe['total_restricted_stock_diff'] = dataframe.total_stock_value - dataframe.restricted_stock \n",
    "    dataframe['to_poi_ratio'] = dataframe.from_this_person_to_poi/ dataframe.to_messages\n",
    "    dataframe['from_poi_ratio'] = dataframe.from_poi_to_this_person / dataframe.from_messages\n",
    "    dataframe['payments_to_stock_ratio'] = dataframe.total_payments / dataframe.total_stock_value\n",
    "    # fill inf or NaN ratios in new columns with 0.0\n",
    "    dataframe.loc[dataframe.to_poi_ratio.isna(), 'to_poi_ratio'] = 0.0\n",
    "    dataframe.loc[dataframe.from_poi_ratio.isna(), 'from_poi_ratio'] = 0.0\n",
    "    dataframe.loc[(dataframe.payments_to_stock_ratio == inf) | \n",
    "                  (dataframe.payments_to_stock_ratio.isna()), 'payments_to_stock_ratio'] = 0.0 \n",
    "    return dataframe\n",
    "\n",
    "def clean_data(dataframe):\n",
    "    \"\"\"\n",
    "    Add new columns, convert \"poi\" from boolean to binary, convert pandas objects to floats (fill NaN with 0.0), \n",
    "    split X and y data\n",
    "    \"\"\"\n",
    "    nan_dict = {} # Create dictionary to store total NaN values by column label\n",
    "    for col in dataframe.columns:\n",
    "        # Adjust string \"NaN\" values to the numpy representation, count total nan values for each label and save to nan_dict\n",
    "        dataframe.loc[dataframe[col] == 'NaN', col] = np.nan\n",
    "        nan_dict.update({col : dataframe[col].isnull().sum()})\n",
    "        if col == 'email_address':\n",
    "            pass\n",
    "        elif col == 'poi':\n",
    "            dataframe[col] = dataframe[col].astype(int) # Convert poi to binary\n",
    "        else:\n",
    "            dataframe[col] = dataframe[col].astype(float) # All other numeric values as floats\n",
    "            dataframe[col] = dataframe[col].fillna(0.0) # Data imputation (fill missing data)   \n",
    "    # Sort nan dict, and print\n",
    "    nan_dict = dict(sorted(nan_dict.items(), key = lambda item: item[1]))\n",
    "    print('\\nTOTAL NAN VALUES BY LABEL:\\n')\n",
    "    for k in nan_dict.keys():\n",
    "        print(k, ':', nan_dict[k])\n",
    "    dataframe = new_columns(dataframe) # Add new columns\n",
    "    # Create separate variables for X and y\n",
    "    print('\\nDataset after cleaning data and adding new features:\\n')\n",
    "    dataframe.info()\n",
    "    X_features = [feature for feature in dataframe.columns if feature not in ['poi', 'email_address']]\n",
    "    X_data = dataframe[X_features]\n",
    "    y_data = dataframe.poi\n",
    "    return X_data, y_data\n",
    "\n",
    "# Convert dos linefeed to unix, return new .pkl file name (\"_unix\" appended)\n",
    "dataset_file = d2ux(\"final_project_dataset.pkl\") \n",
    "\n",
    "# Load the dictionary containing the dataset\n",
    "with open(dataset_file, \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Remove outliers\n",
    "remove_outliers(data_dict)\n",
    "\n",
    "# Read data to dataframe for easier data manipulation, and clean data\n",
    "df = to_dataframe(data_dict)\n",
    "# Print the total number of people in the dataset after removing outliers\n",
    "print(f'\\nThe total number of people in the dataset after removing outliers: {len(df)}')\n",
    "# Print the total number of POIs, and non-POIs\n",
    "print(f'Total Persons of Interest: {df.poi.sum()}\\nTotal who are not Persons of Interest: {len(df) - df.poi.sum()}')\n",
    "X, y = clean_data(df)\n",
    "\n",
    "print('\\nSample of data after data cleaning, and prior to final feature selection and standardization:')\n",
    "pd.concat([y, X], axis = 1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a variety of classifiers to achieve better than 0.3 precision and recall\n",
    "\n",
    "# Choose if data is going to be scaled, and if/how many features will be selected\n",
    "def scale_selectk(X_data, y_data, scale = True, selectk = True, k = 'all'):\n",
    "    \"\"\"\n",
    "    Specifies whether to scale the data, and which best columns to keep\n",
    "    \"\"\"\n",
    "    # Scale data (keep as dataframe) if scale = True\n",
    "    if scale == True:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_data = pd.DataFrame(data = scaler.fit_transform(X_data),\n",
    "                              columns = X_data.columns,\n",
    "                              index = X_data.index)\n",
    "    # Keep selected X features if selectk = True\n",
    "    if selectk == True:\n",
    "        best = SelectKBest(f_classif, k = k).fit(X_data, y_data)\n",
    "        X_mask = best.get_support()\n",
    "        X_kept_cols = X_data.columns[X_mask].tolist()\n",
    "        print('The kept features are:', X_kept_cols)\n",
    "        print('The total number of kept features are:', len(X_kept_cols))\n",
    "        print('The feature scores are:')\n",
    "        for c, b in zip(X_data.columns, best.scores_):\n",
    "            print(c, ':', b)\n",
    "        X_data = X_data[X_kept_cols]\n",
    "    return X_data\n",
    "\n",
    "# Print different performance metrics for a classifier\n",
    "def metrics(predictions, y_val, estimator_descr = 'estimator'):\n",
    "    \"\"\"\n",
    "    Prints the accuracy, f1_score, recall, and precision for a given prediction\n",
    "    \"\"\"\n",
    "    print(f'\\nAccuracy Score for {estimator_descr}: {accuracy_score(predictions, y_val) * 100}%')\n",
    "    print(f'F1 Score for {estimator_descr}: {f1_score(predictions, y_val) * 100}%')\n",
    "    print(f'Precision Score for {estimator_descr}: {precision_score(predictions, y_val) * 100}%')\n",
    "    print(f'Recall Score for {estimator_descr}: {recall_score(predictions, y_val) * 100}%\\n')\n",
    "    \n",
    "# Define performance reporting function to print the best performing parameters when using GridSearchCV\n",
    "def best_performance(classifier):\n",
    "    \"\"\"\n",
    "    Reports performance and parameters of the best classifier of a parameter search/CV\n",
    "    \"\"\"\n",
    "    print('Best Parameters: ' + str(classifier.best_params_) + '\\n')\n",
    "    print('Best Score: ' + str(classifier.best_score_))\n",
    "\n",
    "# Optimize estimator using cross-validation\n",
    "def EstimatorCV(X_data, y_data, pipeline, params): \n",
    "    # define parameters to test using grid search\n",
    "    estimator = GridSearchCV(pipeline, \n",
    "                             param_grid = params, \n",
    "                             cv = 5, \n",
    "                             scoring = make_scorer(f1_score, pos_label = 1),\n",
    "                             verbose = True, \n",
    "                             n_jobs = -1)\n",
    "    # find best training parameters\n",
    "    best_estimator = estimator.fit(X_data, y_data)\n",
    "    best_performance(best_estimator)\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score for Naive Bayes: 65.51724137931035%\n",
      "F1 Score for Naive Bayes: 0.0%\n",
      "Precision Score for Naive Bayes: 0.0%\n",
      "Recall Score for Naive Bayes: 0.0%\n",
      "\n",
      "\n",
      "Accuracy Score for Naive Bayes with PCA: 58.620689655172406%\n",
      "F1 Score for Naive Bayes with PCA: 0.0%\n",
      "Precision Score for Naive Bayes with PCA: 0.0%\n",
      "Recall Score for Naive Bayes with PCA: 0.0%\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'pca__n_components': 5, 'pca__whiten': True}\n",
      "\n",
      "Best Score: 0.45999999999999996\n",
      "\n",
      "Accuracy Score for Naive Bayes with optimized PCA parameters: 79.3103448275862%\n",
      "F1 Score for Naive Bayes with optimized PCA parameters: 25.0%\n",
      "Precision Score for Naive Bayes with optimized PCA parameters: 25.0%\n",
      "Recall Score for Naive Bayes with optimized PCA parameters: 25.0%\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22  3]\n",
      " [ 3  1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "# Scale using Standard Scaler\n",
    "nb_scaler = StandardScaler()\n",
    "nb_X = nb_scaler.fit_transform(X)\n",
    "\n",
    "# Split into training/testing sets (Best performance when there is no scaling/removing features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(nb_X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "pca = PCA()\n",
    "nbc = GaussianNB()\n",
    "\n",
    "nb_pipeline = Pipeline([('pca', pca),\n",
    "                         ('nbc', nbc)])\n",
    "\n",
    "# Naive Bayes scores with no parameter optimization, no PCA:\n",
    "nbc = nbc.fit(X_train, y_train)\n",
    "nbc_pred = nbc.predict(X_test)\n",
    "metrics(nbc_pred, y_test, 'Naive Bayes')\n",
    "\n",
    "# Naive Bayes scores with no parameter optimization, but with PCA\n",
    "nbc_w_pca = nb_pipeline.fit(X_train, y_train)\n",
    "nbc_w_pca_pred = nbc_w_pca.predict(X_test)\n",
    "metrics(nbc_w_pca_pred, y_test, 'Naive Bayes with PCA')\n",
    "\n",
    "# Naive Bayes cross-validation\n",
    "nb_param_grid = {'pca__n_components' : [None, 2, 5, 10],\n",
    "                 'pca__whiten' : [True, False]}\n",
    "\n",
    "nb_clf = EstimatorCV(X_train, y_train, nb_pipeline, nb_param_grid)\n",
    "nb_clf_pred = nb_clf.predict(X_test)\n",
    "metrics(nb_clf_pred, y_test, 'Naive Bayes with optimized PCA parameters')\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, nb_clf_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kept features are: ['salary', 'bonus', 'total_stock_value', 'exercised_stock_options', 'total_restricted_stock_diff']\n",
      "The total number of kept features are: 5\n",
      "The feature scores are:\n",
      "salary : 18.575703268041824\n",
      "to_messages : 1.6988243485808527\n",
      "deferral_payments : 0.21705893033950568\n",
      "total_payments : 8.873835255516232\n",
      "loan_advances : 7.242730396536017\n",
      "bonus : 21.06000170753659\n",
      "restricted_stock_deferred : 0.06498431172370998\n",
      "deferred_income : 11.595547659731226\n",
      "total_stock_value : 24.467654047526405\n",
      "expenses : 6.234201140506746\n",
      "from_poi_to_this_person : 5.344941523147335\n",
      "exercised_stock_options : 25.09754152873551\n",
      "from_messages : 0.1641644982342872\n",
      "other : 4.246153540676069\n",
      "from_this_person_to_poi : 2.426508127242881\n",
      "long_term_incentive : 10.072454529369448\n",
      "shared_receipt_with_poi : 8.746485532129082\n",
      "restricted_stock : 9.346700791051369\n",
      "director_fees : 2.107655943276089\n",
      "total_restricted_stock_diff : 25.661492064167202\n",
      "to_poi_ratio : 4.1690838152893965\n",
      "from_poi_ratio : 5.209650220581801\n",
      "payments_to_stock_ratio : 0.6498790250165934\n",
      "\n",
      "Accuracy Score for Random Forest: 79.3103448275862%\n",
      "F1 Score for Random Forest: 0.0%\n",
      "Precision Score for Random Forest: 0.0%\n",
      "Recall Score for Random Forest: 0.0%\n",
      "\n",
      "\n",
      "Accuracy Score for Random Forest with PCA: 86.20689655172413%\n",
      "F1 Score for Random Forest with PCA: 0.0%\n",
      "Precision Score for Random Forest with PCA: 0.0%\n",
      "Recall Score for Random Forest with PCA: 0.0%\n",
      "\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'pca__n_components': 2, 'pca__whiten': True, 'rfc__class_weight': 'balanced', 'rfc__min_samples_leaf': 6}\n",
      "\n",
      "Best Score: 0.4565079365079365\n",
      "\n",
      "Accuracy Score for Random Forest with PCA and optimized parameters: 72.41379310344827%\n",
      "F1 Score for Random Forest with PCA and optimized parameters: 42.85714285714285%\n",
      "Precision Score for Random Forest with PCA and optimized parameters: 75.0%\n",
      "Recall Score for Random Forest with PCA and optimized parameters: 30.0%\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18  7]\n",
      " [ 1  3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Scale = True, select 5 features\n",
    "rf_X = scale_selectk(X, y, scale = True, selectk = True, k = 5)\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(rf_X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "pca = PCA()\n",
    "rfc = RandomForestClassifier(criterion = 'entropy',\n",
    "                             random_state = 0,\n",
    "                             max_features = 'auto',\n",
    "                             warm_start = True)\n",
    "\n",
    "rf_pipeline = Pipeline([('pca', pca), \n",
    "                        ('rfc', rfc)])\n",
    "\n",
    "# Random Forest scores with no parameter optimization, no PCA\n",
    "rfc = rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "metrics(rfc_pred, y_test, 'Random Forest')\n",
    "\n",
    "# Random Forest scores with no parameter optimization, but with PCA\n",
    "rfc_w_pca = rf_pipeline.fit(X_train, y_train)\n",
    "rfc_w_pca_pred = rfc_w_pca.predict(X_test)\n",
    "metrics(rfc_w_pca_pred, y_test, 'Random Forest with PCA')\n",
    "\n",
    "# Random forest cross-validation\n",
    "rf_param_grid = {'pca__n_components' : [None, 2, 5],\n",
    "                 'pca__whiten' : [True, False],\n",
    "                 'rfc__min_samples_leaf': [1, 2, 4, 6],\n",
    "                 'rfc__class_weight' : [None, 'balanced']}\n",
    "\n",
    "rf_clf = EstimatorCV(X_train, y_train, rf_pipeline, rf_param_grid)\n",
    "rf_clf_pred = rf_clf.predict(X_test)\n",
    "metrics(rf_clf_pred, y_test, 'Random Forest with PCA and optimized parameters')\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, rf_clf_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kept features are: ['salary', 'to_messages', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'from_poi_to_this_person', 'exercised_stock_options', 'from_messages', 'other', 'from_this_person_to_poi', 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees', 'total_restricted_stock_diff', 'to_poi_ratio', 'from_poi_ratio', 'payments_to_stock_ratio']\n",
      "The total number of kept features are: 23\n",
      "The feature scores are:\n",
      "salary : 18.575703268041824\n",
      "to_messages : 1.6988243485808527\n",
      "deferral_payments : 0.21705893033950568\n",
      "total_payments : 8.873835255516232\n",
      "loan_advances : 7.242730396536017\n",
      "bonus : 21.06000170753659\n",
      "restricted_stock_deferred : 0.06498431172370998\n",
      "deferred_income : 11.595547659731226\n",
      "total_stock_value : 24.467654047526405\n",
      "expenses : 6.234201140506746\n",
      "from_poi_to_this_person : 5.344941523147335\n",
      "exercised_stock_options : 25.09754152873551\n",
      "from_messages : 0.1641644982342872\n",
      "other : 4.246153540676069\n",
      "from_this_person_to_poi : 2.426508127242881\n",
      "long_term_incentive : 10.072454529369448\n",
      "shared_receipt_with_poi : 8.746485532129082\n",
      "restricted_stock : 9.346700791051369\n",
      "director_fees : 2.107655943276089\n",
      "total_restricted_stock_diff : 25.661492064167202\n",
      "to_poi_ratio : 4.1690838152893965\n",
      "from_poi_ratio : 5.209650220581801\n",
      "payments_to_stock_ratio : 0.6498790250165934\n",
      "\n",
      "Accuracy Score for SVM: 86.20689655172413%\n",
      "F1 Score for SVM: 0.0%\n",
      "Precision Score for SVM: 0.0%\n",
      "Recall Score for SVM: 0.0%\n",
      "\n",
      "\n",
      "Accuracy Score for SVM with PCA: 86.20689655172413%\n",
      "F1 Score for SVM with PCA: 0.0%\n",
      "Precision Score for SVM with PCA: 0.0%\n",
      "Recall Score for SVM with PCA: 0.0%\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Parameters: {'pca__n_components': None, 'svc__C': 0.1, 'svc__kernel': 'linear'}\n",
      "\n",
      "Best Score: 0.4076190476190476\n",
      "\n",
      "Accuracy Score for Random Forest with PCA and optimized parameters: 72.41379310344827%\n",
      "F1 Score for Random Forest with PCA and optimized parameters: 20.0%\n",
      "Precision Score for Random Forest with PCA and optimized parameters: 25.0%\n",
      "Recall Score for Random Forest with PCA and optimized parameters: 16.666666666666664%\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  5]\n",
      " [ 3  1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "\n",
    "# Scale = True, select all features\n",
    "svm_X = scale_selectk(X, y, scale = True, selectk = True, k = 'all')\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(svm_X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "pca = PCA(whiten = True)\n",
    "svc = SVC(random_state = 0,\n",
    "          probability = True,\n",
    "          decision_function_shape = 'ovo')\n",
    "\n",
    "svc_pipeline = Pipeline([('pca', pca), \n",
    "                         ('svc', svc)])\n",
    "\n",
    "# SVM scores with no parameter optimization, no PCA\n",
    "svc = svc.fit(X_train, y_train)\n",
    "svc_pred = svc.predict(X_test)\n",
    "metrics(svc_pred, y_test, 'SVM')\n",
    "\n",
    "# SVM scores with no parameter optimization, but with PCA\n",
    "svc_w_pca = svc_pipeline.fit(X_train, y_train)\n",
    "svc_w_pca_pred = svc_w_pca.predict(X_test)\n",
    "metrics(svc_w_pca_pred, y_test, 'SVM with PCA')\n",
    "\n",
    "# SVM cross-validation\n",
    "svm_param_grid = {'pca__n_components' : [None, 2, 5],\n",
    "                  'svc__C' : [0.1, 10, 50, 100],\n",
    "                  'svc__kernel' : ['linear', 'rbf', 'sigmoid']}\n",
    "\n",
    "svm_clf = EstimatorCV(X_train, y_train, svc_pipeline, svm_param_grid)\n",
    "svm_clf_pred = svm_clf.predict(X_test)\n",
    "metrics(svm_clf_pred, y_test, 'Random Forest with PCA and optimized parameters')\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, svm_clf_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kept features are: ['bonus', 'total_stock_value', 'exercised_stock_options', 'total_restricted_stock_diff']\n",
      "The total number of kept features are: 4\n",
      "The feature scores are:\n",
      "salary : 18.575703268041824\n",
      "to_messages : 1.6988243485808527\n",
      "deferral_payments : 0.21705893033950568\n",
      "total_payments : 8.873835255516232\n",
      "loan_advances : 7.242730396536017\n",
      "bonus : 21.06000170753659\n",
      "restricted_stock_deferred : 0.06498431172370998\n",
      "deferred_income : 11.595547659731226\n",
      "total_stock_value : 24.467654047526405\n",
      "expenses : 6.234201140506746\n",
      "from_poi_to_this_person : 5.344941523147335\n",
      "exercised_stock_options : 25.09754152873551\n",
      "from_messages : 0.1641644982342872\n",
      "other : 4.246153540676069\n",
      "from_this_person_to_poi : 2.426508127242881\n",
      "long_term_incentive : 10.072454529369448\n",
      "shared_receipt_with_poi : 8.746485532129082\n",
      "restricted_stock : 9.346700791051369\n",
      "director_fees : 2.107655943276089\n",
      "total_restricted_stock_diff : 25.661492064167202\n",
      "to_poi_ratio : 4.1690838152893965\n",
      "from_poi_ratio : 5.209650220581801\n",
      "payments_to_stock_ratio : 0.6498790250165934\n",
      "\n",
      "Accuracy Score for Gradient Booster: 72.41379310344827%\n",
      "F1 Score for Gradient Booster: 0.0%\n",
      "Precision Score for Gradient Booster: 0.0%\n",
      "Recall Score for Gradient Booster: 0.0%\n",
      "\n",
      "\n",
      "Accuracy Score for Gradient Booster with PCA: 86.20689655172413%\n",
      "F1 Score for Gradient Booster with PCA: 0.0%\n",
      "Precision Score for Gradient Booster with PCA: 0.0%\n",
      "Recall Score for Gradient Booster with PCA: 0.0%\n",
      "\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'gbc__max_depth': 1, 'gbc__n_estimators': 250, 'gbc__subsample': 0.4}\n",
      "\n",
      "Best Score: 0.6342857142857143\n",
      "\n",
      "Accuracy Score for Random Forest with PCA and optimized parameters: 89.65517241379311%\n",
      "F1 Score for Random Forest with PCA and optimized parameters: 66.66666666666666%\n",
      "Precision Score for Random Forest with PCA and optimized parameters: 75.0%\n",
      "Recall Score for Random Forest with PCA and optimized parameters: 60.0%\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23  2]\n",
      " [ 1  3]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boost Classifier\n",
    "\n",
    "\n",
    "# Scale = True, select 4 features\n",
    "gb_X = scale_selectk(X, y, scale = True, selectk = True, k = 4)\n",
    "\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gb_X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "pca = PCA(n_components = None)\n",
    "gbc = GradientBoostingClassifier(loss = 'exponential',\n",
    "                                 random_state = 0, \n",
    "                                 max_features = 'auto',\n",
    "                                 warm_start = True)\n",
    "\n",
    "gbc_pipeline = Pipeline([('pca', pca), \n",
    "                         ('gbc', gbc)])\n",
    "\n",
    "# Gradient Boost Classifier scores with no parameter optimization, no PCA\n",
    "gbc = gbc.fit(X_train, y_train)\n",
    "gbc_pred = gbc.predict(X_test)\n",
    "metrics(gbc_pred, y_test, 'Gradient Booster')\n",
    "\n",
    "# Gradient Boost Classifier scores with no parameter optimization, but with PCA\n",
    "gbc_w_pca = gbc_pipeline.fit(X_train, y_train)\n",
    "gbc_w_pca_pred = gbc_w_pca.predict(X_test)\n",
    "metrics(gbc_w_pca_pred, y_test, 'Gradient Booster with PCA')\n",
    "\n",
    "# Gradient Boost Classifier cross-validation\n",
    "gbc_param_grid = {'gbc__n_estimators' : [100, 250, 300, 350, 400, 450],\n",
    "                  'gbc__subsample' : [0.1, 0.25, 0.3, 0.4, 0.5, 1.0],\n",
    "                  'gbc__max_depth' : [1, 2, 3]}\n",
    "\n",
    "gb_clf = EstimatorCV(X_train, y_train, gbc_pipeline, gbc_param_grid)\n",
    "gb_clf_pred = gb_clf.predict(X_test)\n",
    "metrics(gb_clf_pred, y_test, 'Random Forest with PCA and optimized parameters')\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, gb_clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump classifier, dataset, and feature list\n",
    "\n",
    "# Save best parameters to clf variable for final model\n",
    "pca = PCA(n_components = None)\n",
    "gbc = GradientBoostingClassifier(loss = 'exponential',\n",
    "                                 max_depth = 1,\n",
    "                                 n_estimators = 250,\n",
    "                                 random_state = 0, \n",
    "                                 max_features = 'auto',\n",
    "                                 warm_start = True,\n",
    "                                 subsample = 0.4)\n",
    "gbc_pipeline = Pipeline([('pca', pca), \n",
    "                         ('gbc', gbc)])\n",
    "\n",
    "clf = gbc_pipeline.fit(X_train, y_train)\n",
    "# Format dataset and feature_list for final submission\n",
    "dataset = pd.concat([y.astype(bool), gb_X], axis = 1).to_dict(orient = 'index')\n",
    "feature_list = ['poi'] + gb_X.columns.tolist()\n",
    "\n",
    "# Dump to pickles\n",
    "dump_classifier_and_data(clf, dataset, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA()),\n",
      "                ('gbc',\n",
      "                 GradientBoostingClassifier(loss='exponential', max_depth=1,\n",
      "                                            max_features='auto',\n",
      "                                            n_estimators=250, random_state=0,\n",
      "                                            subsample=0.4, warm_start=True))])\n",
      "\tAccuracy: 0.91840\tPrecision: 0.71532\tRecall: 0.64450\tF1: 0.67806\tF2: 0.65752\n",
      "\tTotal predictions: 15000\tTrue positives: 1289\tFalse positives:  513\tFalse negatives:  711\tTrue negatives: 12487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final test of model (Goal is at least 0.3 for recall and accuracy scores)\n",
    "\n",
    "from tester import test_classifier\n",
    "test_classifier(clf, dataset, feature_list, folds = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
